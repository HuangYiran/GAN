{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glob\n",
    "主要查找符合特定规则的文件路径名。他用的匹配符比较简单，主要只用三个：$*， ？（单个）， []（范围）$<br>\n",
    "- glob.glob返回所有匹配的文件路径列表。参数是parameter：glob.glob(r\"E:\\Picture\\*\\*.jpg\")\n",
    "- glob.iglob() 获取一个可遍历的对象，和glob.glob的区别在于，iglob返回的是一个iterator。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from __future__ import division\n",
    "导入python未来支持的语言特征division(精确除法)，当我们没有在程序中导入该特征时，\"/\"操作符执行的是截断除法(Truncating Division),当我们导入精确除法之后，\"/\"执行的是精确除法\n",
    "\n",
    "#### math.ceil(x)\n",
    "return the ceiling of x as an Integral. This is the smallest integer >=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from glob import glob\n",
    "from six.moves import xrange\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DCGAN(object):\n",
    "    \"\"\"\n",
    "    模型类，功能应该包括提供建模，训练， 评价接口\n",
    "    \"\"\"\n",
    "    def __init__(self, sess, input_height=108, input_width=108, crop=True,\n",
    "            batch_size=64, sample_num=64, output_height=64, output_width=64,\n",
    "            y_dim=None, z_dim=100, gf_dim=64, df_dim=64, \n",
    "            gfc_dim=1024, dfc_dim=1024, c_dim=3, dataset_name='default',\n",
    "            input_fname_pattern='*.jpg', checkpoint_dir=None, sample_dir=None):\n",
    "        \"\"\"\n",
    "        init即初始化，即初始化模型的一些super params并加载或构建模型。\n",
    "        参数一般包括：\n",
    "        1. 因为层数是固定的所以无法作为参数，但每层的神经元数量，激活函数是可以做为参数的\n",
    "        2. 为了能够断点续航，session是必须的。checkpoint_dir一般是当前路径。可以作为参数，同样的还有数据的路径，包括训练和测试的数据。\n",
    "        3. 上面参数中不理解的是crop,sample_num,sample_dir, dataset_name参数，不知道是指什么参数\n",
    "        \"\"\"\n",
    "        self.sess = sess\n",
    "        self.crop = crop\n",
    "        \n",
    "        self.input_height = intput_height\n",
    "        self.input_width = input_width\n",
    "        self.output_height = output_height\n",
    "        self.output_width = output_width\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.sample_num = sample_num\n",
    "        \n",
    "        self.y_dim = y_dim\n",
    "        self.z_dim = z_dim\n",
    "        \n",
    "        self.gf_dim = gf_dim\n",
    "        self.df_dim = df_dim\n",
    "        \n",
    "        self.gfc_dim = gfc_dim\n",
    "        self.dfc_dim = dfc_dim\n",
    "        \n",
    "        # batch normalization: deals with poor initialization helps gradient flow\n",
    "        self.d_bn1 = batch_norm(name='d_bn1')\n",
    "        self.d_bn2 = batch_norm(name='d_bn2')\n",
    "        \n",
    "        # 不理解y_dim指的是什么，不存在y_dim就加一个bn也不知道是什么意思？？\n",
    "        if not self.y_dim:\n",
    "            self_d_bn3 = batch_norm(name='d_bn3')\n",
    "        \n",
    "        # 为什么这里又是从0开始了，这个逻辑也是不认的\n",
    "        self.g_bn0 = batch_norm(name='g_bn0')\n",
    "        self.g_bn1 = batch_norm(name='g_bn1')\n",
    "        self.g_bn2 = batch_norm(name='g_bn2')\n",
    "        \n",
    "        if not self.y_dim:\n",
    "            self_g_bn3 = batch_norm(name='g_bn3')\n",
    "        \n",
    "        self.dataset_name = dataset_name\n",
    "        self.input_fname_pattern = input_fname_pattern\n",
    "        self.checkpoint_dir = checkpoint_dir\n",
    "        \n",
    "        # 这里的X大写y小写， 究竟有什么意思呢？？\n",
    "        if self.dataset_name == 'mnist':\n",
    "            self.data_X, self.data_y = self.load_mnist()\n",
    "            self.c_dim = self.data_X[0].shape[-1] # channel\n",
    "        \"\"\"\n",
    "        else:\n",
    "            self.data = glob(os.path.join(\"./data\", self.dataset_name, self.input_fname_pattern))\n",
    "            imreadImg = imread(self.data[0])\n",
    "            if len(imreadImg.shape) >= 3:\n",
    "                self.c_dim = imread(self.data[0].shape[-1]) #imreadImg.shape不一样吗？\n",
    "            else:\n",
    "                self.c_dim = 1\n",
    "        self.grayscale = (self.c_dim == 1)\n",
    "        \"\"\"\n",
    "        self.build_model()\n",
    "\n",
    "    def discriminator(self, image, y=None, reuse=False):\n",
    "        with tf.variable_scope(\"discriminator\") as scope:\n",
    "            if reuse: # 不知道是干嘛用的？\n",
    "                scope.reuse_variables()\n",
    "            if not self.y_dim:\n",
    "                h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv'))\n",
    "                h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name='d_h1_conv')))\n",
    "                h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name='d_h2_conv')))\n",
    "                h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name='d_h2_conv')))\n",
    "                h4 = linear(tf.reshape(h3, [self.batch_size, -1]), 1, 'd_h4_li') #为什么要分batch输入\n",
    "                \n",
    "                return tf.nn.sigmoid(h4), h4\n",
    "            else:\n",
    "                # 下面的不理解为什么要进行concat，他的好处是什么\n",
    "                yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])\n",
    "                x = conv_cond_concat(image, yb)\n",
    "                \n",
    "                h0 = lrelu(conv2d(x, self.c_dim + self.df_dim, name = 'd_h0_conv'))\n",
    "                h0 = conv_cond_concat(h0, yb)\n",
    "                \n",
    "                h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim + self.y_dim, name='d_h1_conv')))\n",
    "                h1 = tf.reshape(h1, [self.batch_size, -1])\n",
    "                h1 = concat([h1, y1], 1)\n",
    "                \n",
    "                h2 = lrelu(self.d_bn2(linear(h1, self.dfc_dim, 'd_h2_lin')))\n",
    "                h2 = concat([h2, y], 1)\n",
    "                \n",
    "                h3 = linear(h2, 1, 'd_h3_lin')\n",
    "                \n",
    "                return tf.nn.sigmoid(h3), h3\n",
    "    \n",
    "    def generator(self, z, y = None):\n",
    "        with tf.variable_scope(\"generator\") as scope:\n",
    "            if not self.y_dim:\n",
    "                s_h, s_w = self.output_height, self.output_width\n",
    "                s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2)\n",
    "                s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2)\n",
    "                s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2)\n",
    "                s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2)\n",
    "                \n",
    "                self.z_, self.h0_w, self.h0_b = linear(z, self.gf_dim*8*s_h16*s_w16, 'g_h0_lin', with_w=True)\n",
    "                self.h0 = tf.reshape(self.z_, [-1, s_h16, s_w16, self.gf_dim*8])\n",
    "                \n",
    "                h0 = tf.nn.relu(self.g_bn0(self.h0))\n",
    "                \n",
    "                self.h1, self.h1_w, self.h1_b = deconv2d(h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4],\n",
    "                                                        name='g_h1', with_w=True)\n",
    "                h1 = tf.nn.relu(self.g_bn1(self.h1))\n",
    "                \n",
    "                self.h2, self.h2_w, self.h2_b = deconv2d(h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2],\n",
    "                                                        name='g_h2', with_w=True)\n",
    "                h2 = tf.nn.relu(self.g_bn2(self.h2))\n",
    "                \n",
    "                self.h3, self.h3_w, self.h3_b = deconv2d(h2, [self.batch_size, s_h2, s_w2, self.gf_dim],\n",
    "                                                        name='g_h3', with_w=True)\n",
    "                h3 = tf.nn.relu(self.g_bn3(self.h3))\n",
    "                \n",
    "                h4, self.h4_w, self.h4_b = deconv2d(h3, [self.batch_size, s_h, s_w, self.c_dim],\n",
    "                                                   name='g_h4', with_w=True)\n",
    "                return tf.nn.tanh(h4)\n",
    "            else:\n",
    "                s_h, s_w = self.output_height, self.output_width\n",
    "                s_h2, s_w2 = int(s_h/2), int(s_w/2) # 为什么不向上取整了？？？\n",
    "                s_h4, s_w4 = int(s_h/4), int(s_w/4)\n",
    "                \n",
    "                yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])\n",
    "                z = concat([z, y], 1) #不是扩展channel维了吗？？\n",
    "                \n",
    "                h0 = tf.nn.relu(self.g_bn0(linear(z, self.gfc_dim, 'g_h0_lin')))\n",
    "                h0 = concat([h0, y], 1)\n",
    "                \n",
    "                h1 = tf.nn.relu(self.g_bn1(linear(h0, self.gf_dim*2*s_h4*s_w4, )))\n",
    "                h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2])\n",
    "                h1 = conv_cond_concat(h1, yb)\n",
    "                \n",
    "                h2 = tf.nn.relu(self.g_bn2(deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim*2], name='g_h2')))\n",
    "                h2 = conv_cond_concat(h2, yb)\n",
    "                \n",
    "                return tf.nn.sigmoid(deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name='g_h3'))\n",
    "                \n",
    "                \n",
    "        \n",
    "    def conv_out_size_same(size, stride):\n",
    "        return int(math.ceil(float(size)/float(stride)))\n",
    "    \n",
    "    def load_mnist(self):\n",
    "        \"\"\"\n",
    "        读mnist，这里是按unit8（字节）读，而且前几位是图片信息，这里没有做图片信息分析，是因为图片信息已知。\n",
    "        mnist集前16字节是图片信息，前4为验证码，随后是图片数目，cols， hors\n",
    "        \"\"\"\n",
    "        data_dir = os.path.join(\"./data\", self.dataset_name)\n",
    "        \n",
    "        # 没关文件\n",
    "        fd = open(os.path.join(data_dir, 'train-images-idx3-ubyte'))\n",
    "        loaded = np.fromfile(file=fd, dtype=np.unit8) #都是unit8吗\n",
    "        trX = loaded[16:].reshape((-1, 28, 28, 1)).astype(np.float) #16是怎么来的\n",
    "        \n",
    "        fd = open(os.path.join(data_dir, 'train-labels-idx1-ubyte'))\n",
    "        loaded = np.fromfile(file=fd, dtype=np.unit8)\n",
    "        trY = loaded[8:].reshape((-1)).astype(np.float) #这个reshape有什么意义吗？？一维转一维？？\n",
    "        \n",
    "        fd = open(os.path.join(data_dir, 'test-images-idx3-ubyte'))\n",
    "        loaded = np.fromfile(file=fd, dtype=np.unit8)\n",
    "        teX = loaded[16:].reshape((-1, 28, 28, 1)).astype(np.float)\n",
    "        \n",
    "        fd = open(os.path.join(data_dir, 'test-labels-idx1-ubyte'))\n",
    "        loaded = np.fromfile(file=fd, dtpye=np.unit8)\n",
    "        teY = loaded[8:].reshape((-1)).astype(np.float)\n",
    "        \n",
    "        # 必须吗，那X呢\n",
    "        trY = np.asarray(trY)\n",
    "        teY = np.asarray(teY)\n",
    "        \n",
    "        # 把数据都连一起了，不测试了？？\n",
    "        X = np.concatenae((trX, teX), axis = 0)\n",
    "        y = np.concatenate((trY, teY), axis = 0).astype(np.int)\n",
    "        \n",
    "        # 醉了，还能这样shuffle pair，也是够了\n",
    "        seed = 547\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(X)\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(y)\n",
    "        \n",
    "        y_vec = np.zeros((len(y), self.y_dim), dtype=np.float) #len(y)行，y_dim为标签的维度\n",
    "        \n",
    "        # 猜测：因为目标是数值识别，所以这里y_dim可能是9，然后，下面这步就是one hot了\n",
    "        for i, label in enumerate(y):\n",
    "            y_vec[i, y[i]] = 1.0 #可以用label的\n",
    "        \n",
    "        #/255是为啥？？？算normalization？\n",
    "        return X/255., y_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### np.random.seed(seed=None)\n",
    "This method is called when `RandomState` is initialized. It can be\n",
    "called again to re-seed the generator.\n",
    "\n",
    "#### sigmoid_cross_entropy_with_logits(_sentinel=None, labels=None, logits=None, name=None)\n",
    "Computes sigmoid cross entropy given logits.<br>\n",
    "Measures the probability error in discrete classification tasks in which each class is independent and not mutually exclusive. For instance, one could perform multilabel classification where a picture can contain both an elephant and a dog at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "?np.zeros()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.concat(concat_di, values, name)\n",
    "tf.concat(concat_dim, values, name='concat')<br>\n",
    "concat_dim是tensor连接的方向（维度），values是要连接的tensor链表，name是操作名。cancat_dim维度可以不一样，其他维度的尺寸必须一样。下面举两个例子：两个二维tensor连接，两个三维tensor连接。<br>\n",
    "##### 两个二维tensor连接：<br>\n",
    "concat_dim：0表示行，1表示列<br>\n",
    "t1 = [[1,2,3], [4,5,6]]<br>\n",
    "t2 = [[7,8,9], [10,11,12]]<br>\n",
    "tf.concat(0, [t1, t2]) ==> [[1,2,3], [4,5,6], [7,8,9], [10,11,12]]<br>\n",
    "tf.concat(1, [t1, t2]) ==> [[1,2,3,7,8, 9], [4,5,6,10,11, 12]]<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "# 考虑到版本问题， 做一下协调\n",
    "try:\n",
    "    image_summary = tf.image_summary\n",
    "    scalar_summary = tf.scalar_summary\n",
    "    histogram_summary = tf.histogram_summary\n",
    "    merge_summary = tf.merge_summary\n",
    "    SummaryWriter = tf.train.SummaryWriter\n",
    "except:\n",
    "    image_summary = tf.summary.image\n",
    "    scalar_summary = tf.summary.scalar\n",
    "    histogram_summary = tf.summary.histogram\n",
    "    merge_summary = tf.summary.merge\n",
    "    SummaryWriter = tf.summary.FileWriter\n",
    "\n",
    "if \"concat_v2\" in dir(tf):\n",
    "    def concat(tensors, axis, *args, **kwargs):\n",
    "        return concat_v2(tensors, axis, *args, **kwargs)\n",
    "else:\n",
    "    def concat(tensors, axis, *args, **kwargs):\n",
    "        return concat(tensors, axis, *args, **kwargs)\n",
    "\n",
    "# 建batch_norm类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建batch_norm类\n",
    "batch_norm就是batch normalization<br>\n",
    "tf.contrib.layers.batch_norm(input, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, is_training=True, scope=\"bn\")<br>\n",
    "- inputs: A tensor with 2 or more dimensions, where the first dimension has batch_size. The normalization is over all but the last dimension if data_format is NHWC and the second dimension if data_format is NCHW.<br>\n",
    "- decay: Decay for the moving average. Reasonable values for decay are close to 1.0, typically in the multiple-nines range: 0.999, 0.99, 0.9, etc. Lower decay value (recommend trying decay=0.9) if model experiences reasonably good training performance but poor validation and/or test performance. Try zero_debias_moving_mean=True for improved stability.<br>\n",
    "- updates_collections: Collections to collect the update ops for computation. The updates_ops need to be executed with the train_op. If None, a control dependency would be added to make sure the updates are computed in place.<br>\n",
    "- epsilon: Small float added to variance to avoid dividing by zero.<br>\n",
    "- scope: Optional scope for variable_scope<br>\n",
    "- is_training: Whether or not the layer is in training mode. In training mode it would accumulate the statistics of the moments into moving_mean and moving_variance using an exponential moving average with the given decay. When it is not in training mode then it would use the values of the moving_mean and the moving_variance.<br>\n",
    "- scope: Optional scope for variable_scope<br>\n",
    "\n",
    "该方法返回的是一个 tensor，使用示例如下：<br>\n",
    "x = tf.placeholder(tf.float32, [64, 28,28,1])<br>\n",
    "w= tf.truncated_normal([5,5,1,32], stddev=0.1)<br>\n",
    "b = tf.constant(0.1, shape=[32])<br>\n",
    "h = tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME') + b<br>\n",
    "h_bn = tf.contrib.layers.batch_norm(h, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, is_training=True, scope=\"bn\")<br>\n",
    "h_r = tf.nn.relu(h_bn)<br>\n",
    "主要解释来自知乎问题“深度学习中 Batch Normalization为什么效果好”<br>\n",
    "1. What is BN?\n",
    "顾名思义，batch normalization嘛，就是“批规范化”咯。Google在ICML文中描述的非常清晰，即在每次SGD时，通过mini-batch来对相应的activation做规范化操作，使得结果（输出信号各个维度）的均值为0，方差为1. 而最后的“scale and shift”操作则是为了让因训练所需而“刻意”加入的BN能够有可能还原最初的输入（即当），从而保证整个network的capacity。（有关capacity的解释：实际上BN可以看作是在原模型上加入的“新操作”，这个新操作很大可能会改变某层原来的输入。当然也可能不改变，不改变的时候就是“还原原来输入”。如此一来，既可以改变同时也可以保持原输入，那么模型的容纳能力（capacity）就提升了。）<br>\n",
    "关于DNN中的normalization，大家都知道白化（whitening），只是在模型训练过程中进行白化操作会带来过高的计算代价和运算时间。因此本文提出两种简化方式：1）直接对输入信号的每个维度做规范化（“normalize each scalar feature independently”）；2）在每个mini-batch中计算得到mini-batch mean和variance来替代整体训练集的mean和variance. 这便是Algorithm 1.\n",
    "3. Where to use BN?\n",
    "BN可以应用于网络中任意的activation set。文中还特别指出在CNN中，BN应作用在非线性映射前,即对x=Wu+b做规范化。\n",
    "4. Why BN?\n",
    "好了，现在才是重头戏－－为什么要用BN？BN work的原因是什么？那BN到底是什么原理呢？说到底还是为了防止“梯度弥散”。\n",
    "5. When to use BN?\n",
    "OK，说完BN的优势，自然可以知道什么时候用BN比较好。例如，在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。\n",
    "\n",
    "####  __call__方法\n",
    "关于__call__方法，不得不先提到一个概念，就是可调用对象callable，我们平时自定义函数、内置函数和类都属于可调用对象，但凡是可以把一堆括号（）应用到某个对象身上都可称为可调用对象判断对象是否为可调用对象可以用函数callable()<br>\n",
    "如果在类中实现了__call__方法，那么实例对象也将称为一个可调用对象，实例对象也可以像函数一样作为可调用对象来用。<br>\n",
    "那么什么场景用得上呢？这个要结合类的特性来说，类可以记录数据（属性），而函数不行（闭包某种意义上也可以），利用这种特性可以实现基于类的装饰器，在类里面记录状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class batch_norm(object):\n",
    "    def __init__(self, epsilon=1e-5, momentum=0.9, name=\"batch_norm\"):\n",
    "        \"\"\"\n",
    "        不是很理解，为什么要加这个\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name):\n",
    "            self.epsilon = epsilon\n",
    "            self.momentum = momentum\n",
    "            self.name = name\n",
    "    \n",
    "    def __call__(self, x, train=True):\n",
    "        return tf.contrib.layers.batch_norm(x, \n",
    "                                           decay = self.momentum,\n",
    "                                           updates_collections=None,\n",
    "                                           epsilon=self.epsilon,\n",
    "                                           scale=True,\n",
    "                                           is_training=True,\n",
    "                                           scope=self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_cond_concat(x,y):\n",
    "    \"\"\"\n",
    "    x.shape: [self.batch_size, image_cols, image_hor, channel]\n",
    "    y.shape: [self.batch_size, 1, 1, self.y_dim]\n",
    "    y*tf.ones[]的结果，形状上来讲核x是差不多的，不过x中图片上不同位置上的值是不同的，然而在这里则都是应该根据y值来决定，对应目标的\n",
    "    channel（还是当成channel来想象，比较好理解）上，全是1，而被其他的都是0.\n",
    "    concat所以结果就成了[self.batch_size, image_cols, image_hor, channel+self.y+dim]。也就是说，channel 变多了。\n",
    "    前几个原图片的channel，后几个为新加入的。\n",
    "    \"\"\"\n",
    "    x_shapes = x.get_shape()\n",
    "    y_shapes = y.get_shape()\n",
    "    return concat([x, y*tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 记几个方法\n",
    "##### get_variable(name, shape=None, dtype=None, initializer=None, regularizer=None, trainable=True, collections=None, caching_device=None, partitioner=None, validate_shape=True, use_resource=None, custom_getter=None)\n",
    "Gets an existing variable with these parameters or create a new one.\n",
    "\n",
    "##### tf.truncated_normal_initializer\n",
    "Initializer that generates a truncated normal distribution.<br>\n",
    "These values are similar to values from a random_normal_initializer except that values more than two standard deviations from the mean are discarded and re-drawn. This is the recommended initializer for neural network weights and filters.<br>\n",
    "__init__(<br>\n",
    "    mean=0.0,<br>\n",
    "    stddev=1.0,<br>\n",
    "    seed=None,<br>\n",
    "    dtype=tf.float32<br>\n",
    ")\n",
    "\n",
    "##### tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\n",
    "Computes a 2-D convolution given 4-D input and filter tensors.<br>\n",
    "Given an input tensor of shape [batch, in_height, in_width, in_channels] and a filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "\n",
    "##### tf.nn.bias_add(value, bias, data_format=None, name=None)\n",
    "Adds bias to value.<br>\n",
    "This is (mostly) a special case of tf.add where bias is restricted to 1-D. Broadcasting is supported, so value may have any number of dimensions. Unlike tf.add, the type of bias is allowed to differ from value in the case where both types are quantized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(input_, output_dim, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02, name=\"conv2d\"):\n",
    "    \"\"\"filter: [height, width, output_channels, input_channels]\"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim],\n",
    "                            initializer = tf.truncated_normal_initializer(stddev=stddev))\n",
    "        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\n",
    "        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "        conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n",
    "        return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 继续几个方法\n",
    "##### tf.nn.conv2d_transpose(value, filter, output_shape, strides, padding='SAME', data_format='NHWC', name=None)\n",
    "The transpose of conv2d: This operation is sometimes called \"deconvolution\" after Deconvolutional Networks, but is actually the transpose (gradient) of conv2d rather than an actual deconvolution.\n",
    "\n",
    "##### tf.nn.conv2d\n",
    "是旧的版本（0.7.0以前），现在看不到了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deconv2d(input_, output_shape, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02, name=\"deconv2d\", with_w=False):\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n",
    "                            initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "        try:\n",
    "            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1])\n",
    "        except:\n",
    "            deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape,\n",
    "                                    strides=[1, d_h, d_w, 1])\n",
    "        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
    "        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n",
    "        if with_w:\n",
    "            return deconv, w, biases\n",
    "        else:\n",
    "            return deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    \"\"\"leaky relu: 大于零保持，小于零打折\"\"\"\n",
    "    return max(x, leak*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):\n",
    "    shape = input_.get_shape().as_list() #****不该相乘吗？？\n",
    "    with tf.variable_scope(scope or \"Linear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [shape[1], output_size], tf.float32,\n",
    "                                 initializer = tf.random_normal_initializer(stddev=stddev))\n",
    "        bias = tf.get_variable(\"bias\", [output_size],\n",
    "                              initializer=tf.constant_initializer(bias_start))\n",
    "        if with_w:\n",
    "            return tf.matmul(input_, matrix) + bias, matrix, bias\n",
    "        else:\n",
    "            return tf.matmul(input_, matrix) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x = tf.placeholder(tf.float32, [1, 1])\n",
    "y = tf.placeholder(tf.float32, [1, 1])\n",
    "h0 = li(x, 10, 'in')\n",
    "h0 = tf.nn.sigmoid(h0)\n",
    "tf.contrib.layers.batch_norm(h0, \n",
    "                           decay = 0.9,\n",
    "                           updates_collections=None,\n",
    "                           epsilon=1e-5,\n",
    "                           scale=True,\n",
    "                           is_training=True)\n",
    "out = li(h0, 1, 'out')\n",
    "squared_deltas = tf.square(out - y)\n",
    "loss = tf.reduce_sum(squared_deltas)\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "X = list(np.random.normal(0, 10, 10000))\n",
    "noise = list(np.random.normal(0, 1, 10000))\n",
    "counter = 1\n",
    "\n",
    "for label, i in enumerate(X):\n",
    "    a = np.matrix([i])\n",
    "    b = np.matrix([i*i+ noise[label]])\n",
    "    sess.run(train, {x: a, y: b})\n",
    "    counter = counter + 1\n",
    "    if counter%100000 == 0:\n",
    "        print(a)\n",
    "        print(sess.run(h0, {x: a}))\n",
    "        print(sess.run(out, {x: a}))\n",
    "        print(sess.run(loss, {x: a, y: b}))\n",
    "\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def li(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):\n",
    "    shape = input_.shape  #****不该相乘吗？？\n",
    "    with tf.variable_scope(scope or \"Linear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [shape[1], output_size], tf.float32,\n",
    "                                 initializer = tf.random_normal_initializer(stddev=stddev))\n",
    "        bias = tf.get_variable(\"bias\", [output_size],\n",
    "                              initializer=tf.constant_initializer(bias_start))\n",
    "        return tf.matmul(input_, matrix) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lrelu(x, leak = 0.2):\n",
    "    return max(x, tf.matmul([[leak]], x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"MatMul:0\", shape=(1, 10), dtype=float32)\n",
      "[[-0.04569849 -0.03433954 -0.03440236 -0.03189389  0.00359198  0.01259415\n",
      "   0.01644778 -0.00417289  0.00840337  0.02925177]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "matrix = li2()\n",
    "print(matrix)\n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "print(sess.run(matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-64b61e65583e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "X = list(np.random.normal(0, 10, 10000))\n",
    "noise = list(np.random.normal(0, 1, 10000))\n",
    "\n",
    "for i, j in .(X, noise):\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  6.93290543, -14.95265141,   4.46539758, ...,  -9.89825101,\n",
       "        20.13026969,  10.51131503])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.normal(0, 10, 10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(max(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
