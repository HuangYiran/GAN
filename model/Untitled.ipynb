{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Glob\n",
    "主要查找符合特定规则的文件路径名。他用的匹配符比较简单，主要只用三个：$*， ？（单个）， []（范围）$<br>\n",
    "- glob.glob返回所有匹配的文件路径列表。参数是parameter：glob.glob(r\"E:\\Picture\\*\\*.jpg\")\n",
    "- glob.iglob() 获取一个可遍历的对象，和glob.glob的区别在于，iglob返回的是一个iterator。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### from __future__ import division\n",
    "导入python未来支持的语言特征division(精确除法)，当我们没有在程序中导入该特征时，\"/\"操作符执行的是截断除法(Truncating Division),当我们导入精确除法之后，\"/\"执行的是精确除法\n",
    "\n",
    "#### math.ceil(x)\n",
    "return the ceiling of x as an Integral. This is the smallest integer >=x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from glob import glob\n",
    "from six.moves import xrange\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "#from ops import *\n",
    "#from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model (self, sess, input_height=108, input_width=108, crop=True,\n",
    "           batch_size=64, sample_num=64, output_height=64, output_width=64,\n",
    "           y_dim=None, z_dim=100, gf_dim=64, df_dim=64, \n",
    "           gfc_dim=1024, dfc_dim=1024, c_dim=3, dataset_name='default',\n",
    "           input_fname_pattern='*.jpg', checkpoint_dir=None, sample_dir=None):\n",
    "    \"\"\"\n",
    "    这里为了方便运行，写成了方法，一般是作为类考虑的，下面参数讨论，是从类的角度出发，model类一般需要包括建模，运行，get_batch，Evaluation等方法\n",
    "    参数一般包括：\n",
    "    1. 因为层数是固定的所以无法作为参数，但每层的神经元数量，激活函数是可以做为参数的\n",
    "    2. 为了能够断点续航，session是必须的。checkpoint_dir一般是当前路径。可以作为参数，同样的还有数据的路径，包括训练和测试的数据。\n",
    "    3. 上面参数中不理解的是crop,sample_num,sample_dir参数，不知道是指什么参数\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.concat(concat_di, values, name)\n",
    "tf.concat(concat_dim, values, name='concat')<br>\n",
    "concat_dim是tensor连接的方向（维度），values是要连接的tensor链表，name是操作名。cancat_dim维度可以不一样，其他维度的尺寸必须一样。下面举两个例子：两个二维tensor连接，两个三维tensor连接。<br>\n",
    "##### 两个二维tensor连接：<br>\n",
    "concat_dim：0表示行，1表示列<br>\n",
    "t1 = [[1,2,3], [4,5,6]]<br>\n",
    "t2 = [[7,8,9], [10,11,12]]<br>\n",
    "tf.concat(0, [t1, t2]) ==> [[1,2,3], [4,5,6], [7,8,9], [10,11,12]]<br>\n",
    "tf.concat(1, [t1, t2]) ==> [[1,2,3,7,8, 9], [4,5,6,10,11, 12]]<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "\n",
    "# 考虑到版本问题， 做一下协调\n",
    "try:\n",
    "    image_summary = tf.image_summary\n",
    "    scalar_summary = tf.scalar_summary\n",
    "    histogram_summary = tf.histogram_summary\n",
    "    merge_summary = tf.merge_summary\n",
    "    SummaryWriter = tf.train.SummaryWriter\n",
    "except:\n",
    "    image_summary = tf.summary.image\n",
    "    scalar_summary = tf.summary.scalar\n",
    "    histogram_summary = tf.summary.histogram\n",
    "    merge_summary = tf.summary.merge\n",
    "    SummaryWriter = tf.summary.FileWriter\n",
    "\n",
    "if \"concat_v2\" in dir(tf):\n",
    "    def concat(tensors, axis, *args, **kwargs):\n",
    "        return concat_v2(tensors, axis, *args, **kwargs)\n",
    "else:\n",
    "    def concat(tensors, axis, *args, **kwargs):\n",
    "        return concat(tensors, axis, *args, **kwargs)\n",
    "\n",
    "# 建batch_norm类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建batch_norm类\n",
    "batch_norm就是batch normalization<br>\n",
    "tf.contrib.layers.batch_norm(input, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, is_training=True, scope=\"bn\")<br>\n",
    "- inputs: A tensor with 2 or more dimensions, where the first dimension has batch_size. The normalization is over all but the last dimension if data_format is NHWC and the second dimension if data_format is NCHW.<br>\n",
    "- decay: Decay for the moving average. Reasonable values for decay are close to 1.0, typically in the multiple-nines range: 0.999, 0.99, 0.9, etc. Lower decay value (recommend trying decay=0.9) if model experiences reasonably good training performance but poor validation and/or test performance. Try zero_debias_moving_mean=True for improved stability.<br>\n",
    "- updates_collections: Collections to collect the update ops for computation. The updates_ops need to be executed with the train_op. If None, a control dependency would be added to make sure the updates are computed in place.<br>\n",
    "- epsilon: Small float added to variance to avoid dividing by zero.<br>\n",
    "- scope: Optional scope for variable_scope<br>\n",
    "- is_training: Whether or not the layer is in training mode. In training mode it would accumulate the statistics of the moments into moving_mean and moving_variance using an exponential moving average with the given decay. When it is not in training mode then it would use the values of the moving_mean and the moving_variance.<br>\n",
    "- scope: Optional scope for variable_scope<br>\n",
    "\n",
    "该方法返回的是一个 tensor，使用示例如下：<br>\n",
    "x = tf.placeholder(tf.float32, [64, 28,28,1])<br>\n",
    "w= tf.truncated_normal([5,5,1,32], stddev=0.1)<br>\n",
    "b = tf.constant(0.1, shape=[32])<br>\n",
    "h = tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME') + b<br>\n",
    "h_bn = tf.contrib.layers.batch_norm(h, decay=0.9, updates_collections=None, epsilon=1e-5, scale=True, is_training=True, scope=\"bn\")<br>\n",
    "h_r = tf.nn.relu(h_bn)<br>\n",
    "主要解释来自知乎问题“深度学习中 Batch Normalization为什么效果好”<br>\n",
    "1. What is BN?\n",
    "顾名思义，batch normalization嘛，就是“批规范化”咯。Google在ICML文中描述的非常清晰，即在每次SGD时，通过mini-batch来对相应的activation做规范化操作，使得结果（输出信号各个维度）的均值为0，方差为1. 而最后的“scale and shift”操作则是为了让因训练所需而“刻意”加入的BN能够有可能还原最初的输入（即当），从而保证整个network的capacity。（有关capacity的解释：实际上BN可以看作是在原模型上加入的“新操作”，这个新操作很大可能会改变某层原来的输入。当然也可能不改变，不改变的时候就是“还原原来输入”。如此一来，既可以改变同时也可以保持原输入，那么模型的容纳能力（capacity）就提升了。）<br>\n",
    "关于DNN中的normalization，大家都知道白化（whitening），只是在模型训练过程中进行白化操作会带来过高的计算代价和运算时间。因此本文提出两种简化方式：1）直接对输入信号的每个维度做规范化（“normalize each scalar feature independently”）；2）在每个mini-batch中计算得到mini-batch mean和variance来替代整体训练集的mean和variance. 这便是Algorithm 1.\n",
    "3. Where to use BN?\n",
    "BN可以应用于网络中任意的activation set。文中还特别指出在CNN中，BN应作用在非线性映射前,即对x=Wu+b做规范化。\n",
    "4. Why BN?\n",
    "好了，现在才是重头戏－－为什么要用BN？BN work的原因是什么？那BN到底是什么原理呢？说到底还是为了防止“梯度弥散”。\n",
    "5. When to use BN?\n",
    "OK，说完BN的优势，自然可以知道什么时候用BN比较好。例如，在神经网络训练时遇到收敛速度很慢，或梯度爆炸等无法训练的状况时可以尝试BN来解决。另外，在一般使用情况下也可以加入BN来加快训练速度，提高模型精度。\n",
    "\n",
    "####  __call__方法\n",
    "关于__call__方法，不得不先提到一个概念，就是可调用对象callable，我们平时自定义函数、内置函数和类都属于可调用对象，但凡是可以把一堆括号（）应用到某个对象身上都可称为可调用对象判断对象是否为可调用对象可以用函数callable()<br>\n",
    "如果在类中实现了__call__方法，那么实例对象也将称为一个可调用对象，实例对象也可以像函数一样作为可调用对象来用。<br>\n",
    "那么什么场景用得上呢？这个要结合类的特性来说，类可以记录数据（属性），而函数不行（闭包某种意义上也可以），利用这种特性可以实现基于类的装饰器，在类里面记录状态。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class batch_norm(object):\n",
    "    def __init__(self, epsilon=1e-5, momentum=0.9, name=\"batch_norm\"):\n",
    "        \"\"\"\n",
    "        不是很理解，为什么要加这个\n",
    "        \"\"\"\n",
    "        with tf.variable_scope(name):\n",
    "            self.epsilon = epsilon\n",
    "            self.momentum = momentum\n",
    "            self.name = name\n",
    "    \n",
    "    def __call__(self, x, train=True):\n",
    "        return tf.contrib.layers.batch_norm(x, \n",
    "                                           decay = self.momentum,\n",
    "                                           updates_collections=None,\n",
    "                                           epsilon=self.epsilon,\n",
    "                                           scale=True,\n",
    "                                           is_training=train,\n",
    "                                           scope=self.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_cond_concat(x,y):\n",
    "    \"\"\"Concatenate conditioning vector on feature map axis: 这个看不懂啊\"\"\"\n",
    "    x_shapes = x.get_shape()\n",
    "    y_shapes = y.get_shape()\n",
    "    return concat([x, y*tf.ones([x_shapes[0], x_shapes[1], x_shapes[2], y_shapes[3]])], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 记几个方法\n",
    "##### get_variable(name, shape=None, dtype=None, initializer=None, regularizer=None, trainable=True, collections=None, caching_device=None, partitioner=None, validate_shape=True, use_resource=None, custom_getter=None)\n",
    "Gets an existing variable with these parameters or create a new one.\n",
    "\n",
    "##### tf.truncated_normal_initializer\n",
    "Initializer that generates a truncated normal distribution.<br>\n",
    "These values are similar to values from a random_normal_initializer except that values more than two standard deviations from the mean are discarded and re-drawn. This is the recommended initializer for neural network weights and filters.<br>\n",
    "__init__(<br>\n",
    "    mean=0.0,<br>\n",
    "    stddev=1.0,<br>\n",
    "    seed=None,<br>\n",
    "    dtype=tf.float32<br>\n",
    ")\n",
    "\n",
    "##### tf.nn.conv2d(input, filter, strides, padding, use_cudnn_on_gpu=None, data_format=None, name=None)\n",
    "Computes a 2-D convolution given 4-D input and filter tensors.<br>\n",
    "Given an input tensor of shape [batch, in_height, in_width, in_channels] and a filter / kernel tensor of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "\n",
    "##### tf.nn.bias_add(value, bias, data_format=None, name=None)\n",
    "Adds bias to value.<br>\n",
    "This is (mostly) a special case of tf.add where bias is restricted to 1-D. Broadcasting is supported, so value may have any number of dimensions. Unlike tf.add, the type of bias is allowed to differ from value in the case where both types are quantized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(input_, output_dim, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02, name=\"conv2d\"):\n",
    "    \"\"\"filter: [height, width, output_channels, input_channels]\"\"\"\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w', [k_h, k_w, input_.get_shape()[-1], output_dim],\n",
    "                            initializer = tf.truncated_normal_initializer(stddev=stddev))\n",
    "        conv = tf.nn.conv2d(input_, w, strides=[1, d_h, d_w, 1], padding='SAME')\n",
    "        biases = tf.get_variable('biases', [output_dim], initializer=tf.constant_initializer(0.0))\n",
    "        conv = tf.reshape(tf.nn.bias_add(conv, biases), conv.get_shape())\n",
    "        return conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 继续几个方法\n",
    "##### tf.nn.conv2d_transpose(value, filter, output_shape, strides, padding='SAME', data_format='NHWC', name=None)\n",
    "The transpose of conv2d: This operation is sometimes called \"deconvolution\" after Deconvolutional Networks, but is actually the transpose (gradient) of conv2d rather than an actual deconvolution.\n",
    "\n",
    "##### tf.nn.conv2d\n",
    "是旧的版本（0.7.0以前），现在看不到了\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deconv2d(input_, output_shape, k_h=5, k_w=5, d_h=2, d_w=2, stddev=0.02, name=\"deconv2d\", with_w=False):\n",
    "    with tf.variable_scope(name):\n",
    "        w = tf.get_variable('w', [k_h, k_w, output_shape[-1], input_.get_shape()[-1]],\n",
    "                            initializer=tf.random_normal_initializer(stddev=stddev))\n",
    "        try:\n",
    "            deconv = tf.nn.conv2d_transpose(input_, w, output_shape=output_shape, strides=[1, d_h, d_w, 1])\n",
    "        except:\n",
    "            deconv = tf.nn.deconv2d(input_, w, output_shape=output_shape,\n",
    "                                    strides=[1, d_h, d_w, 1])\n",
    "        biases = tf.get_variable('biases', [output_shape[-1]], initializer=tf.constant_initializer(0.0))\n",
    "        deconv = tf.reshape(tf.nn.bias_add(deconv, biases), deconv.get_shape())\n",
    "        if with_w:\n",
    "            return deconv, w, biases\n",
    "        else:\n",
    "            return deconv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    \"\"\"leaky relu: 大于零保持，小于零打折\"\"\"\n",
    "    return maximum(x, leak*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear(input_, output_size, scope=None, stddev=0.02, bias_start=0.0, with_w=False):\n",
    "    shape = input_.get_shape().as_list() #****不该相乘吗？？\n",
    "    with tf.variable_scope(scope or \"Linear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [shape[1], output_size], tf.float32,\n",
    "                                 tf.random_normal_initializer(stddev=stddev))\n",
    "        bias = tf.get_variable(\"bias\", [output_size],\n",
    "                              initializer=tf.constant_initializer(bias_start))\n",
    "        if with_w:\n",
    "            return tf.matmul(input_, matrix) + bias, matrix, bias\n",
    "        else:\n",
    "            return tf.matmul(input_, matrix) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
